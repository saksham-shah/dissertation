{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.28.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-base\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"rpn\": True,\n",
    "    \"dataset\": \"mawps\",\n",
    "    \"epochs\": 2,\n",
    "    \"weight_decay\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwp_to_dict(mwp):\n",
    "    return {\n",
    "        \"id\": mwp.id,\n",
    "        \"question\": mwp.question,\n",
    "        \"equation\": mwp.equation,\n",
    "        \"answer\": mwp.answer,\n",
    "        \"numbers\": mwp.numbers,\n",
    "    }\n",
    "\n",
    "def train_test_split(data, test_size=0.1):\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    boundary = math.floor(len(data) * (1 - test_size))\n",
    "    train = data[:len(data) - boundary - 1]\n",
    "    test = data[boundary:]\n",
    "    return { \"train\": train, \"test\": test }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWPDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, inputs, targets):\n",
    "    self.inputs = inputs\n",
    "    self.targets = targets\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx], device=device) for key, val in self.inputs.items()}\n",
    "    item['labels'] = torch.tensor(self.targets['input_ids'][idx], device=device)\n",
    "    return item\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    # mwps, _, _ = load_data(config)\n",
    "    mwps = prepare_training_data(config['dataset'])\n",
    "    print(f\"Num mwps: {len(mwps)}\")\n",
    "    data = list(map(mwp_to_dict, mwps))\n",
    "\n",
    "    inputs = train_test_split([mwp[\"question\"] for mwp in data])\n",
    "    targets = train_test_split([mwp[\"equation\"] for mwp in data])\n",
    "    mwps = train_test_split(data)\n",
    "\n",
    "    return inputs, targets, mwps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_data(tokeniser, inputs, targets):\n",
    "    max_input_length = 1024\n",
    "    max_target_length = 64\n",
    "\n",
    "    tokenised_inputs = {\n",
    "        \"train\": tokeniser(inputs[\"train\"], max_length=max_input_length, truncation=True),\n",
    "        \"test\": tokeniser(inputs[\"test\"], max_length=max_input_length, truncation=True),\n",
    "    }\n",
    "\n",
    "    tokenised_targets = {\n",
    "        \"train\": tokeniser(targets[\"train\"], max_length=max_target_length, truncation=True),\n",
    "        \"test\": tokeniser(targets[\"test\"], max_length=max_target_length, truncation=True),\n",
    "    }\n",
    "\n",
    "    train_dataset = MWPDataset(tokenised_inputs[\"train\"], tokenised_targets[\"train\"])\n",
    "    test_dataset = MWPDataset(tokenised_inputs[\"test\"], tokenised_targets[\"test\"])\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, model, tokeniser, train_dataset, test_dataset, test_mwps):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        f\"{model_checkpoint}-finetunes-mawps\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=config[\"epochs\"],\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokeniser, model=model)\n",
    "\n",
    "    def compute_metrics(tokeniser, mwps, eval_pred):\n",
    "        print(eval_pred.predictions)\n",
    "        print(eval_pred.predictions.shape)\n",
    "        print(eval_pred.label_ids)\n",
    "        print(eval_pred.label_ids.shape)\n",
    "        print(eval_pred.inputs)\n",
    "        if eval_pred.inputs is not None:\n",
    "            print(eval_pred.inputs.shape)\n",
    "\n",
    "        correct = 0\n",
    "\n",
    "        for i in range(len(eval_pred.predictions)):\n",
    "            mwp = mwps[i]\n",
    "\n",
    "            numbers = list(map(float, mwp['numbers'].split(\",\")))\n",
    "            answer = mwp[\"answer\"]\n",
    "            target = mwp[\"equation\"]\n",
    "\n",
    "            pred_tokens = np.expand_dims(eval_pred.predictions[i], 0)\n",
    "            pred = [tokeniser.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces=False) for token in pred_tokens]\n",
    "            \n",
    "            rpn_exp = infix_to_rpn(pred[0].split(\" \"))\n",
    "            output_ans = eval_rpn(rpn_exp, numbers)\n",
    "\n",
    "            if output_ans is not None and math.isclose(output_ans, answer, rel_tol=1e-4):\n",
    "                # print(\"CORRECT:\", pred[0], \"<>\", target)\n",
    "                correct += 1\n",
    "            # else:\n",
    "            #     print(\"WRONG:\", pred[0], \" @@@ \", target)\n",
    "            \n",
    "        return {\n",
    "            'accuracy': correct / len(eval_pred.predictions)\n",
    "        }\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokeniser,\n",
    "        compute_metrics=partial(compute_metrics, tokeniser, test_mwps),\n",
    "    )\n",
    "\n",
    "    print(\"Training now...\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(model, input_tokens, target, numbers, answer, tokeniser, attempts=3):\n",
    "    for _ in range(attempts):\n",
    "        pred_tokens = model.generate(input_tokens['input_ids'], num_beams=4, max_length=32, early_stopping=True)\n",
    "        pred = [tokeniser.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces=False) for token in pred_tokens]\n",
    "\n",
    "        rpn_exp = infix_to_rpn(pred[0].split(\" \"))\n",
    "        output_ans = eval_rpn(rpn_exp, numbers)\n",
    "\n",
    "        if output_ans is None:\n",
    "            print(\"RETRYING\")\n",
    "            continue\n",
    "\n",
    "        if math.isclose(output_ans, answer, rel_tol=1e-4):\n",
    "            print(\"CORRECT:\", pred[0], \"<>\", target)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"WRONG:\", pred[0], \" @@@ \", target)\n",
    "            return False\n",
    "\n",
    "        # print(pred[0], \"<>\", rpn_exp)\n",
    "        # if rpn_exp is None:\n",
    "        #     print(\"Retrying\")\n",
    "        #     continue\n",
    "\n",
    "        # if pred[0] == target:\n",
    "        #     print(\"CORRECT:\", pred)\n",
    "        #     return True\n",
    "        # else:\n",
    "        #     print(pred[0], \" @@@ \", target)\n",
    "        #     return False\n",
    "    return False\n",
    "\n",
    "def evaluate_accuracy(model, tokeniser, inputs, targets, mwps):\n",
    "    print(\"Evaluating...\")\n",
    "    correct = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i]\n",
    "        target = targets[i]\n",
    "        mwp = mwps[i]\n",
    "\n",
    "        numbers = list(map(float, mwp['numbers'].split(\",\")))\n",
    "        answer = mwp[\"answer\"]\n",
    "\n",
    "        input_tokens = tokeniser([input], max_length=1024, return_tensors='pt')\n",
    "        input_tokens['input_ids'].to(device)\n",
    "\n",
    "        if is_correct(model, input_tokens, target, numbers, answer, tokeniser):\n",
    "            correct += 1\n",
    "\n",
    "        # pred_tokens = model.generate(input_tokens['input_ids'], num_beams=4, max_length=32, early_stopping=True)\n",
    "        # pred = [tokeniser.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces=False) for token in pred_tokens]\n",
    "\n",
    "        # if pred[0] == target:\n",
    "        #     print(\"CORRECT:\", pred)\n",
    "        #     correct += 1\n",
    "        # else:\n",
    "        #     print(pred[0], \" @@@ \", target)\n",
    "\n",
    "    return correct / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num mwps: 1954\n",
      "# train: 195, # test: 196\n",
      "Training now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ee6c112a504dd1bcd06f90a5533e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0426, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03fbc1295f24fb99e7e6e5bd9745c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    0 1640 ...    1    1    1]\n",
      " [   2    0 1640 ...    1    1    1]\n",
      " [   2    0 1640 ...    1    1    1]\n",
      " ...\n",
      " [   2    0 1640 ...    1    1    1]\n",
      " [   2    0 1640 ...    1    1    1]\n",
      " [   2    0 1640 ...    1    1    1]]\n",
      "(196, 20)\n",
      "[[    0 10431   134 ...     1  -100  -100]\n",
      " [    0 10431   288 ...     1  -100  -100]\n",
      " [    0  1640   849 ...     1  -100  -100]\n",
      " ...\n",
      " [    0  1640   849 ...     1  -100  -100]\n",
      " [    0 10431   288 ...     1  -100  -100]\n",
      " [    0 10431   288 ...     1  -100  -100]]\n",
      "(196, 22)\n",
      "None\n",
      "{'eval_loss': 0.8050152659416199, 'eval_accuracy': 0.20918367346938777, 'eval_runtime': 47.1142, 'eval_samples_per_second': 4.16, 'eval_steps_per_second': 0.276, 'epoch': 1.0}\n",
      "{'loss': 0.9254, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516a4be9466845a2bd269e5ec8def970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2     0  1640 ...     1     1     1]\n",
      " [    2     0  1640 ...     1     1     1]\n",
      " [    2     0  1640 ...     1     1     1]\n",
      " ...\n",
      " [    2     0 10431 ...     1     1     1]\n",
      " [    2     0  1640 ...     1     1     1]\n",
      " [    2     0  1640 ...     1     1     1]]\n",
      "(196, 20)\n",
      "[[    0 10431   134 ...     1  -100  -100]\n",
      " [    0 10431   288 ...     1  -100  -100]\n",
      " [    0  1640   849 ...     1  -100  -100]\n",
      " ...\n",
      " [    0  1640   849 ...     1  -100  -100]\n",
      " [    0 10431   288 ...     1  -100  -100]\n",
      " [    0 10431   288 ...     1  -100  -100]]\n",
      "(196, 22)\n",
      "None\n",
      "{'eval_loss': 0.7482272386550903, 'eval_accuracy': 0.22448979591836735, 'eval_runtime': 53.3795, 'eval_samples_per_second': 3.672, 'eval_steps_per_second': 0.244, 'epoch': 2.0}\n",
      "{'train_runtime': 255.6662, 'train_samples_per_second': 1.525, 'train_steps_per_second': 0.102, 'train_loss': 1.483998885521522, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "WRONG: ( #0 - #1 )  @@@  #1 / #0\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #1\n",
      "WRONG: ( #0 - #2 )  @@@  ( #2 * ( #0 - #1 ) )\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #1\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #0\n",
      "WRONG: ( #0 + #1 )  @@@  ( #2 * ( #0 - #1 ) )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m trainer \u001b[39m=\u001b[39m train_model(config, model, tokeniser, train_dataset, test_dataset, mwps[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39m./bart_model_trained\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[39mprint\u001b[39m(evaluate_accuracy(model, tokeniser, inputs[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m], targets[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m], mwps[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "Cell \u001b[1;32mIn[147], line 47\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[1;34m(model, tokeniser, inputs, targets, mwps)\u001b[0m\n\u001b[0;32m     44\u001b[0m input_tokens \u001b[39m=\u001b[39m tokeniser([\u001b[39minput\u001b[39m], max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m input_tokens[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 47\u001b[0m \u001b[39mif\u001b[39;00m is_correct(model, input_tokens, target, numbers, answer, tokeniser):\n\u001b[0;32m     48\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[39m# pred_tokens = model.generate(input_tokens['input_ids'], num_beams=4, max_length=32, early_stopping=True)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m# pred = [tokeniser.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces=False) for token in pred_tokens]\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m#     print(pred[0], \" @@@ \", target)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[147], line 3\u001b[0m, in \u001b[0;36mis_correct\u001b[1;34m(model, input_tokens, target, numbers, answer, tokeniser, attempts)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_correct\u001b[39m(model, input_tokens, target, numbers, answer, tokeniser, attempts\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(attempts):\n\u001b[1;32m----> 3\u001b[0m         pred_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_tokens[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], num_beams\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      4\u001b[0m         pred \u001b[39m=\u001b[39m [tokeniser\u001b[39m.\u001b[39mdecode(token, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m pred_tokens]\n\u001b[0;32m      6\u001b[0m         rpn_exp \u001b[39m=\u001b[39m infix_to_rpn(pred[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1518\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1519\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1520\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1521\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1522\u001b[0m     )\n\u001b[0;32m   1523\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1525\u001b[0m         input_ids,\n\u001b[0;32m   1526\u001b[0m         beam_scorer,\n\u001b[0;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1528\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1529\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1530\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1531\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1532\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1533\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1534\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1535\u001b[0m     )\n\u001b[0;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1538\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:2810\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2806\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 2810\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2811\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2812\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2813\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2815\u001b[0m )\n\u001b[0;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2818\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1396\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1375\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1376\u001b[0m         )\n\u001b[0;32m   1378\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[0;32m   1379\u001b[0m     input_ids,\n\u001b[0;32m   1380\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1393\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1394\u001b[0m )\n\u001b[1;32m-> 1396\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(outputs[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m   1397\u001b[0m lm_logits \u001b[39m=\u001b[39m lm_logits \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   1399\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "inputs, targets, mwps = get_data(config)\n",
    "print(f\"# train: {len(inputs['train'])}, # test: {len(inputs['test'])}\")\n",
    "train_dataset, test_dataset = tokenise_data(tokeniser, inputs, targets)\n",
    "\n",
    "# print(evaluate_accuracy(model, tokeniser, inputs['test'], targets['test'], mwps['test']))\n",
    "\n",
    "trainer = train_model(config, model, tokeniser, train_dataset, test_dataset, mwps['test'])\n",
    "\n",
    "trainer.save_model('./bart_model_trained')\n",
    "\n",
    "print(evaluate_accuracy(model, tokeniser, inputs['test'], targets['test'], mwps['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "WRONG: ( #0 - #1 )  @@@  #1 / #0\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #1\n",
      "WRONG: ( #0 - #2 )  @@@  ( #2 * ( #0 - #1 ) )\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #1\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #0\n",
      "WRONG: ( #0 + #1 )  @@@  ( #2 * ( #0 - #1 ) )\n",
      "WRONG: ( #0 + #1 )  @@@  ( #0 * #1 )\n",
      "WRONG: ( #0 + #3 )  @@@  ( #0 * #2 ) + ( #1 * #3 )\n",
      "WRONG: ( #0 + #1 )  @@@  ( #0 * ( #1 + #2 ) )\n",
      "WRONG: ( #0 - #2 )  @@@  ( ( #0 + #1 ) / #2 )\n",
      "WRONG: ( #0 + #1 )  @@@  ( #0 - #2 )\n",
      "WRONG: ( #0 + #1 )  @@@  ( #2 * ( #0 - #1 ) )\n",
      "WRONG: ( #0 + #1 )  @@@  #0 - #1\n",
      "CORRECT: ( #0 + #1 ) <> ( #0 + #1 )\n",
      "WRONG: ( #0 + #1 )  @@@  #0 + #2\n",
      "WRONG: ( #0 + #1 )  @@@  ( #0 / #0 )\n",
      "WRONG: ( #0 - #1 )  @@@  ( #1 / #0 )\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #1\n",
      "WRONG: ( #0 - #1 )  @@@  #0 + #1\n",
      "WRONG: ( #0 + #1 )  @@@  ( #2 - #1 ) / #0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model2 \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39m./bart_model_trained\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m tokeniser2 \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39m./bart_model_trained\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(evaluate_accuracy(model2, tokeniser2, inputs[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m], targets[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m], mwps[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "Cell \u001b[1;32mIn[147], line 47\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[1;34m(model, tokeniser, inputs, targets, mwps)\u001b[0m\n\u001b[0;32m     44\u001b[0m input_tokens \u001b[39m=\u001b[39m tokeniser([\u001b[39minput\u001b[39m], max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m input_tokens[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 47\u001b[0m \u001b[39mif\u001b[39;00m is_correct(model, input_tokens, target, numbers, answer, tokeniser):\n\u001b[0;32m     48\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[39m# pred_tokens = model.generate(input_tokens['input_ids'], num_beams=4, max_length=32, early_stopping=True)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m# pred = [tokeniser.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces=False) for token in pred_tokens]\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m#     print(pred[0], \" @@@ \", target)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[147], line 3\u001b[0m, in \u001b[0;36mis_correct\u001b[1;34m(model, input_tokens, target, numbers, answer, tokeniser, attempts)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_correct\u001b[39m(model, input_tokens, target, numbers, answer, tokeniser, attempts\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(attempts):\n\u001b[1;32m----> 3\u001b[0m         pred_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_tokens[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], num_beams\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      4\u001b[0m         pred \u001b[39m=\u001b[39m [tokeniser\u001b[39m.\u001b[39mdecode(token, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m pred_tokens]\n\u001b[0;32m      6\u001b[0m         rpn_exp \u001b[39m=\u001b[39m infix_to_rpn(pred[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1518\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1519\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1520\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1521\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1522\u001b[0m     )\n\u001b[0;32m   1523\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1525\u001b[0m         input_ids,\n\u001b[0;32m   1526\u001b[0m         beam_scorer,\n\u001b[0;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1528\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1529\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1530\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1531\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1532\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1533\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1534\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1535\u001b[0m     )\n\u001b[0;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1538\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:2810\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2806\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 2810\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2811\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2812\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2813\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2815\u001b[0m )\n\u001b[0;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2818\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1378\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1375\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1376\u001b[0m         )\n\u001b[1;32m-> 1378\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1379\u001b[0m     input_ids,\n\u001b[0;32m   1380\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1381\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1382\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   1383\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1384\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1385\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1386\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1387\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1388\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1389\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1390\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1391\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1392\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1393\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1394\u001b[0m )\n\u001b[0;32m   1396\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\n\u001b[0;32m   1397\u001b[0m lm_logits \u001b[39m=\u001b[39m lm_logits \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1260\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1253\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1254\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   1255\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1257\u001b[0m     )\n\u001b[0;32m   1259\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1260\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1261\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1262\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1263\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m   1264\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1265\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1266\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1267\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1268\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1269\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1270\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1271\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1272\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1273\u001b[0m )\n\u001b[0;32m   1275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1118\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m   1108\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m   1109\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1116\u001b[0m     )\n\u001b[0;32m   1117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1118\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m   1119\u001b[0m         hidden_states,\n\u001b[0;32m   1120\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1121\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1122\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1123\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1124\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[0;32m   1125\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1126\u001b[0m         ),\n\u001b[0;32m   1127\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1128\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1129\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1130\u001b[0m     )\n\u001b[0;32m   1131\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1133\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:464\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[0;32m    463\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m--> 464\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(hidden_states))\n\u001b[0;32m    465\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    466\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2 = AutoModelForSeq2SeqLM.from_pretrained('./bart_model_trained')\n",
    "tokeniser2 = AutoTokenizer.from_pretrained('./bart_model_trained')\n",
    "\n",
    "print(evaluate_accuracy(model2, tokeniser2, inputs['test'], targets['test'], mwps['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   176,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  306, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   176,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   176,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  288, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288,  111,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  176, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n",
      "tensor([[    2,     0, 10431,   288,   111,   849,   134,     2]])\n",
      "tensor([[    2,     0, 10431,   288,  2055,   849,   134,     2]])\n",
      "tensor([[   2,    0, 1640,  849,  288, 2055,  849,  134, 4839,    2]])\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(targets['test'])):\n",
    "#     t = targets['test'][i]\n",
    "#     i = inputs['']\n",
    "#     label_ids = tokeniser(t, max_length=64, truncation=True)\n",
    "#     pred_tokens = model.generate(input_tokens['input_ids'], num_beams=4, max_length=32, early_stopping=True)\n",
    "#     print(label_ids)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    t = test_dataset[i]\n",
    "    # input_  = t['input_ids']\n",
    "    # print(t['input_ids'])\n",
    "\n",
    "    input = inputs['test'][i]\n",
    "    mwp = mwps['test'][i]\n",
    "\n",
    "    numbers = list(map(float, mwp['numbers'].split(\",\")))\n",
    "    answer = mwp[\"answer\"]\n",
    "\n",
    "    # input_tokens = tokeniser([input], max_length=1024, return_tensors='pt')\n",
    "    # input_tokens.to(device)\n",
    "\n",
    "    # print(t['input_ids'])\n",
    "    # print(input_tokens['input_ids'])\n",
    "\n",
    "    pred_tokens = model.generate(t['input_ids'].unsqueeze(0), num_beams=4, max_length=32, early_stopping=True)\n",
    "    pred = [tokeniser.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces=False) for token in pred_tokens]\n",
    "\n",
    "    rpn_exp = infix_to_rpn(pred[0].split(\" \"))\n",
    "    output_ans = eval_rpn(rpn_exp, numbers)\n",
    "\n",
    "\n",
    "    print(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_accuracy() missing 1 required positional argument: 'mwps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(evaluate_accuracy(model, tokeniser, inputs[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m], targets[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: evaluate_accuracy() missing 1 required positional argument: 'mwps'"
     ]
    }
   ],
   "source": [
    "print(evaluate_accuracy(model, tokeniser, inputs['test'], targets['test']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
